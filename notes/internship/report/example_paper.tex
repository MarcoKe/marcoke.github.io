%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2017} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Feedback-Gated Rectified Linear Units}

\begin{document} 

\twocolumn[
\icmltitle{Feedback-Gated Rectified Linear Units}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Marco Kemmerling}{ma}

\end{icmlauthorlist}

\icmlaffiliation{ma}{University of Maastricht, Maastricht, The Netherlands}
%\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
%\icmlaffiliation{ed}{University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Marco Kemmerling}{m.kemmerling@student.maastrichtuniversity.nl}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract} 
Feedback connections play a prominent role in the human brain but have not received much attention in artificial neural network research. Here, a biologically inspired feedback mechanism which gates rectified linear units is proposed. On the MNIST dataset, autoencoders with feedback show faster convergence, better performance, and more robustness to noise compared to their counterparts without feedback. Some benefits, although less pronounced and less consistent, can be observed when networks with feedback are applied on the CIFAR-10 dataset.\end{abstract} 

\section{Introduction}
\label{introduction}
The brain has served as inspiration for artificial neural networks (ANNs) for decades. While these models are usually heavily simplified compared to the brain, they have seen significant successes in areas such as computer vision, speech recognition, (...), in recent times. 

Despite successes, it is clear that the average human brain is vastly more powerful and versatile than any model used in practice today, and as such it may be useful to investigate how and where exactly the brain and ANNs differ. 

One such discrepancy between ANNs and the brain is the existence of feedback, or top-down connections. While there is clear evidence of prominent feedback connections in the brain, ANNs have overwhelmingly been designed based on the feedforward paradigm. 

Networks that do not work solely on the feedforward principle exist and are called recurrent neural networks (RNNs), but most RNNs used in practice today focus on recurrent connections from one layer to itself (e.g. LSTM networks \cite{hochreiter1997long}), which, while recurrent, arguably do not constitute top-down connections. These networks are typically applied on problems where the input consists of sequence data, where the recurrence allows for memory of previously seen elements of the sequence. 

However, the usefulness of recurrent connections or feedback is not necessarily restricted to sequence data. [expand on this.. ornithology story?]

While the network architectures considered in this paper feature real top down connections, the focus is not on the network topology itself, but on how these top down connections influence the behaviour of single neurons, i.e. a feedback mechanism.

This feedback mechanism is derived from neuroscience literature and examined from two broad angles: (1) Whether the feedback mechanism can in any way improve on standard methods. Relevant metrics include convergence speed and performance quality of the trained network. (2) If examining the feedback's properties and how it behaves under certain conditions (e.g. noisy signals) can offer any insights into what role the feedback might fulfil in the brain. Needless to say, care has to be taken when trying to infer functionality of mechanisms in the brain from simplified artificial networks. Nevertheless, experimentation on artificial models offers an intriguing opportunity, as they are naturally easier to investigate and manipulate than the real brain. 

In the remainder of this paper, some neuroscientific background is explored in section \ref{neocortex} to serve as context for the feedback mechanism, followed by a description of the feedback mechanism itself as it occurs in the brain (section \ref{cellmech}). In section \ref{fgrelu} the mechanism is adapted for use in ANNs and some practical consideration on its use are given in section \ref{inpractice}. The following sections describe a range of experiments with the intention to provide answers to the research questions posed above. 

\section{Neuroscientific Background}  
\label{neocortex}
The neocortex, part of the cerebral cortex, is a part of the brain that evolved in mammals comparatively recently. It comprises around 80\% of the human brain \cite{markram2004} and is therefore often speculated to be responsible for the emergence of higher intelligence. 

The most abundant type of neuron in the neocortex are pyramidal neurons, constituting between 70-85\% of cells. In contrast to the remaining neurons in the neocortex, so called interneurons, which are mostly inhibitory, pyramidal neurons are excitatory \cite{defelipe1992}. 

As the name suggests, pyramidal neurons have a cell body roughly shaped like a pyramid, with a base at the bottom and an apex at the top. Pyramidal neurons have two types of dendrites: basal dendrites, originating at the base, and one apical dendrite, originating at the apex. This apical dendrite terminates in what is called the apical tuft, where heavy branching of apical dendrite occurs. \cite{defelipe1992}. 

These apical and basal dendrites are not just differently located, but also serve different functions. Basal dendrites receive regular feedforward input, while the apical tuft dendrites receive feedback input. 

The neocortex appears to have a distinct structure which is characterised by its organisation into layers as well as columns. The columnar organisation is based on the observation that neurons stacked on top of each other tend to be connected and have similar response properties, while only few connections exist between columns. Columns are hence hypothesised to be a basic functional unit in the cortex, although this is somewhat debated in the neuroscience community \cite{goodhill2002}.

The further organisation into six layers was proposed by Brodman in 1909 [citation]. Layers 1 and 6 are of particular interest here. Layer 1 consists of almost no cell bodies, but mostly connections between axons and the apical dendrites of pyramidal neurons \cite{shipp2007}, i.e. is serves as a connection hub for feedback signals. Layer 6 sends signals to neurons in the thalamus which then in turn sends signals to layer 1 neurons in the same column \cite{shipp2007}, i.e. layers 1 and 6 create a loop where feedback is sent from layer 6 and received by layer 1.  
\subsection{Distal Input to Pyramidal Neurons}
\label{cellmech}
As described above, apical tuft dendrites receive feedback input which appears to modulate the gain of the corresponding neuron \cite{larkum2004}. It is hypothesised that this is a way for the cortex to combine an internal representation of the world with external input, i.e. feedback to a neuron may predict whether this particular should be firing and even small feedforward input may lead the neuron to fire as long as the feedback signal is strong \cite{larkum2013cellular}. 

Taking both feedforward and feedback input into account, the firing rate of a neuron can be modelled as follows \cite{larkum2004}:

\begin{equation} 
	\label{eq:neurogain}
	f = g(\mu_S + \alpha \mu_D + \sigma + f\beta(\mu_D)-\theta)
\end{equation}

where $f$ is the firing rate of the neuron, $g$ the gain, $\mu_S$ the average somatic current (i.e. feedforward input), $\mu_D$ the average distal current (i.e. feedback input), $\alpha$ is an attenuation factor, $\sigma$ represents fluctuations in the current, $\theta$ is the firing threshold, and $\beta(\mu_D)$ is an increasing function of the dendritic mean current which saturates for values above some current threshold.  

\section{Feedback-Gated Rectified Linear Units} 
\label{fgrelu}
The model described in the previous section serves as a basis to derive an activation function which can replace the common rectified linear unit (ReLU) \cite{nair2010rectified}, i.e. $f(x) = max(0, x)$.  

To arrive at a more practical activation function, $g$ and $\theta$ are dropped from equation \ref{eq:neurogain}, since the threshold is modelled through the bias unit and the gain (i.e. slope) of a ReLU is by definition $1$ and can thus be safely dropped. Dropping the summands $\alpha \mu_D$ and $\sigma$ is less justifiable, but since they do not contribute to the core property of gain increase, they will be disregarded here, arriving at the following simplified relationship: 

\begin{equation}
	f = \mu_S + f\beta(\mu_D)
\end{equation}

Removing $f$ from the right hand side: 
\begin{equation} 
\label{eq:rate1}
	f = \frac{1}{1 - \beta(\mu_D)} \mu_S
\end{equation}

What remains is an exact definition of $\beta(\mu_D)$, which, according to \cite{larkum2004}, is ``an increasing function of the dendritic mean current $\mu$ which saturates for values above 1000pA``. In other words, the function is bounded, i.e. the gain cannot be increased to arbitrarily high values. Accordingly, some maximum value $\beta_{max}$ the function can produce and a threshold value $\eta$ which describes when this maximum is reached need to be defined. 
Assuming a piecewise linear model, $\beta(\mu_D)$ is thus defined as follows: 
\begin{equation}
\label{eq:betafunc}
	\beta(\mu_D) = min \bigg(\frac{\beta_{max}}{\eta} \mu_D, \beta_{max}\bigg)
\end{equation} 

As there are no obvious values to assign to $\beta_{max}$ and $\eta$, they are treated as hyperparameters. Since setting $\beta_{max}$ to 1 results in a division by $0$ and a value of $\beta_{max} > 1$ causes a negative slope, $\beta_{max}$ should be smaller than $1$. 

Plugging equation \ref{eq:betafunc} into equation \ref{eq:rate1} yields: 
\begin{equation}
	f = \frac{1}{1 - min(\frac{\beta_{max}}{\eta} \ \mu_D, \beta_{max})} \ \mu_S 
\end{equation}

Since negative values for $\mu_S$ are not taken account, it is replaced with $max(0, \mu_S)$, i.e. the classic ReLU function: 
\begin{equation}
\label{eq:fgrelu}
		f = \frac{max(0, \mu_S)}{1 - min(\frac{\beta_{max}}{\eta} \ \mu_D, \beta_{max})} 
\end{equation}

\subsection{Feedback-Gated ReLUs in Practice}
\label{inpractice}
The feedback path attempts to mimic the top-down path in the brain. As such, the origin of feedback terminating in a layer should be a layer that is higher in the (feedforward) hierarchy. 

Since feedback from higher layers can only be computed if these higher layers have received feedforward input, at least two time steps are needed to incorporate the modified ReLUs into a network. Concretely, some data, e.g. an image is fed into the network twice, where the first pass enables the computation of feedback which can be utilised in the second pass. 
	Although more than two timesteps are not necessary, it is possible to use an arbitrary number of timesteps, which is examined in section \ref{timesteps}.
	
	UNROLLING 
	
Any layer that receives feedback requires an additional set of weights to compute $\mu_D$. Specifically each layer $h_i$ with size $n$ receiving feedback from layer $h_j$ with size $m$ introduces $n \times m$ additional parameters.  
	
Dropout \cite{srivastava2014dropout} should be used by dropping out the same units in all passes.  
If e.g. dropout is only applied on the last pass, the remaining units will still receive signals from dropped out units in previous passes.

In convolutional neural networks \cite{lecun1989generalization}, feedback is implemented on a filter-wise basis, i.e. each neuron does not receive its own unique feedback signal, but rather every filter receives a unique feedback signal that is shared between all units belonging to that filter. 


\section{Experimental Results}
The preceding sections describe a feedback mechanism and how it can be implemented in practice. Here, a range of experiments is performed to observe how this feedback mechanism changes the behaviour of ANNs. Several networks are applied on two datasets, MNIST \cite{lecun2010mnist} and CIFAR-10 \cite{krizhevsky2014cifar}. Specifically, the experiments are designed to answer the research questions posed in the introduction: (1) whether feedback can improve the performance of ANNs, (2) whether observing how the feedback works in artificial models can reveal any clues on what function feedback has in the brain. Sections \ref{constantgain}, \ref{noisyacts}, and \ref{noisycifar} serve to answer the latter question, where section \ref{constantgain} is more of a general analysis of feedback, while sections \ref{noisyacts} and \ref{noisycifar} test whether feedback might increase the networks robustness to noise. The remaining sections are concerned primarily with question (1) in that they test convergence speed and performance quality in various configurations. 

\subsection{MNIST}
\label{mnist}
The MNIST dataset is composed of $28 \times 28$ pixel binary images of handwritten digits, split into $60000$ training and $10000$ test instances \cite{lecun2010mnist}. Each image is associated with one of ten classes representing the digits between $0$ and $9$. 

The models used in the following experiments are based on a (non-convolutional) autoencoder with two encoding and two decoding layers. The input layer has dimension $(1\times784)$, the first encoding layer (E1) outputs data of dimension $(1\times392)$, the second (E2) of dimension $(1\times196)$, the first decoding layer (D1) of dimension $(1\times392)$ and the second decoding layer (D2) restores the data back to its original dimension. Except for the final layer, each layer is followed by a ReLU activation. The final layer makes use of a sigmoid activation function. 

First experiments were performed with only a single feedback connection between the first decoder and the first encoder (see figure \ref{fig:autoenc}). 

\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/AutoEnc.png}
      \caption{Autoencoder with (partial) feedback.}
      \label{fig:autoenc}
  \end{figure}
  
Optimal values for $\eta$ and $\beta_{max}$ were determined by a grid search ($\beta_{max}=0.95, \eta=5$).
 \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/mnist196test.png}
      \caption{Test set loss of autoencoders with and without feedback. The dimension of the second encoding layer is 196. }
      \label{fig:fbvsnofb}
  \end{figure}
  
Figure \ref{fig:fbvsnofb} shows the loss curves for the autoencoder with and without feedback. While the autoencoder with feedback converges noticeably faster, the difference is not ... 

It is conceivable that feedback might have a greater effect if the difficulty of the task is increased. Difficulty is not a well defined term, but reducing the dimension of the second encoding layer (i.e. the bottleneck) can arguably seen as an increase in difficulty. 
The dimension of the second encoding layer is thus reduced to 10 and the experiment is repeated (this modification will persist in all subsequent experiments). Indeed, figure \ref{fig:fbvsnofb10} shows a much larger gap between the autoencoder with feedback and the one without it. 

\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/mnist10test.png}
      \caption{Test set loss of autoencoders with and without feedback. The dimension of the second encoding layer is 10. }
      \label{fig:fbvsnofb10}
  \end{figure}
  
\subsubsection{More Than Two Timesteps}
\label{timesteps}
While at least two timesteps are required to incorporate feedback, it is not clear whether exactly two timesteps should be used or whether $>2$ timesteps can be beneficial. To examine this, autoencoders with 1, 2, 4, 6, and 8 timesteps are trained. 

The results, depicted in figure \ref{fig:timesteps}, show that more than two timesteps yield no or negligible improvement. This may of course be data and/or task dependent. Since MNIST is a fairly simple dataset (binary images, clear separation of background and foreground, etc.), it is not inconceivable that tasks on other datasets may benefit from more than two timesteps. 




\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/timesteps_test.png}
      \caption{Autoencoder performance with varying numbers of timesteps. Each configuration was trained and evaluated 10 times. The curves shown are the averaged losses on the test set.}
      \label{fig:timesteps}
  \end{figure}
 
 
\subsubsection{Comprehensive Feedback}
In the previous experiments, feedback was only sent from one decoding layer to one encoding layer. Naturally, there are many more possible configurations that incorporate further feedback connections. In the following experiment, each layer receives feedback from every layer above it, i.e. every possible top-down connection is present in the network. This will be referred to as \emph{comprehensive} feedback, whereas the previous approach will be referred to as \emph{partial} feedback. 

  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/fullfb.png}
      \caption{Loss on the test set of autoencoders without feedback, partial feedback, and comprehensive feedback. Note that the horizontal axis is different from previous figures, i.e. the training time is longer. }
      \label{fig:fullfeedback}
  \end{figure}
  
  As shown in figure \ref{fig:fullfeedback}, the configuration explained above does not only converge faster than a standard autoencoder, but also settles to a better value than the model with only partial feedback. 
    
 \subsubsection{Feedback vs Constant Gain} 
 \label{constantgain}
 In an effort to gain some understanding on how exactly feedback helps to improve performance, the frequency of different feedback values is examined.  A distinction is made between feedback and gain, where feedback refers to $\mu_D$ and gain refers to $\frac{1}{1 - min(\frac{\beta_{max}}{\eta} \ (\mu_D), \beta_{max})}$. 
 
 Figure \ref{fig:partialhists} shows the data as collected in a network with a single feedback connection. 
 
 \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/constgainpartial.png}
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/constgainpartialgain.png}

      \caption{Distribution of feedback (top) and gain (bottom) values collected in a network with partial feedback over the complete MNIST test set. }
      \label{fig:partialhists}
  \end{figure}
  
  While there are some smaller gain values, the overwhelming majority of values are the maximum gain the network can produce. This raises the question whether there is much benefit to learning feedback or whether it might be similarly beneficial to simply multiply all activation values by a constant. 
  
  This is easily tested by setting the gain of every ReLU in the affected layer to a constant value of 10. 
  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/constgainpartial_loss.png}
      \caption{Placeholder, make sure to update this figure}
      \label{fig:contgainpartialloss}
  \end{figure}
  
  As can be seen in figure \ref{fig:contgainpartialloss}, this does lead to a steeper loss curve than the standard autoencoder, although not quite as steep as that of the autoencoder with actual learned feedback. Further, the performance after training is completed is worse than that of the standard autoencoder. 
  
  Repeating this same experiment for more than one feedback connection yields to results as illustrated in figure \ref{fig:contgainfullloss}. 
  
  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/constgainfull_loss.png}
      \caption{Placeholder, make sure to update this figuree}
      \label{fig:contgainfullloss}
  \end{figure}
  
  In this setup, the simple multiplication by a constant initially converges even faster than the autoencoder with learned feedback. While it does not achieve the same performance as the feedback autoencoder in later stages of training, it is on par with the standard autoencoder's performance. 
  
  Clearly, the effects of feedback cannot be fully explained by this constant gain, but the idea of a constant gain seems to have some merit. 


\subsubsection{Noisy Activations} 
\label{noisyacts}
While noisy signals are usually not an issue in artificial networks, noise in the brain is very prevalent \cite{Faisal2008}. To see whether feedback makes the model more robust to noise, gaussian noise with zero mean and various standard deviations is added to the (pre-)activations of both the network with feedback and the one without it. The networks are only evaluated with added noise, training is performed without noise. Note that in the network with feedback, noise is added to the activations in both passes.    

		
	\begin{equation}
		h = f(W^T x + b + \mathcal{N}(0,\,\sigma^{2})\,)
	\end{equation}
	
	
	As figure \ref{fig:noiseact} shows, the use of feedback significantly increases the network's robustness to noise. While this is not especially useful for machine learning models, it may be part of the reason why the feedback path exists in the brain. 

	\begin{figure}[H]
      \centering
      \includegraphics[width=0.45\textwidth,height=5cm,keepaspectratio]{img/actnoise_recs/input.png}
      \includegraphics[width=0.45\textwidth,height=5cm,keepaspectratio]{img/actnoise_recs/standardrec.png}
      \includegraphics[width=0.45\textwidth,height=5cm,keepaspectratio]{img/actnoise_recs/feedbackrec.png}
      \caption{Gaussian noise with zero mean and standard deviation $\sigma=2.0$ is added to networks with and without feedback. The top row shows input instances to the network, the middle and bottom row show reconstructions of the network without and with feedback (respectively).}
      \label{fig:noiseact}
  \end{figure}
  
  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/actnoise_varstd.png}
      \caption{Gaussian noise with zero mean and varying standard deviations (horizontal) is added to networks with and without feedback. The quality of the reconstruction, as measured by the loss function (vertical axis), with respect to the magnitude of the standard deviation is shown for both networks. }
      \label{fig:noiseact}
  \end{figure}

  
\subsection{CIFAR-10}
The CIFAR-10 dataset is composed of $32\times32$ pixel colour images of various objects, split into 50000 training and 10000 test instances. Each image belongs to one of the following classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck \cite{krizhevsky2014cifar}. 

\subsubsection{Autoencoder}
\label{cifarae}
Similarly to the MNIST experiments, an autoencoder is trained on the CIFAR-10 dataset. Again, the architecture consists of two encoding and two decoding layers. Contrary to MNIST, the encoding/decoding layers used here are convolutional/transposed convolutional layers with 16 $5 \times 5$ filters. 

While the autoencoder with feedback clearly performs better than the one without it, the difference between the two is not as pronounced as it is in the MNIST experiments.   

Curiously, if batch normalisation \cite{ioffe2015batch} is used after the activation functions, feedback cannot improve on the performance of the standard autoencoder. This may suggest that somehow feedback and batch normalisation are interacting in such a way that the feedback is rendered ineffective. 

\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/cifar_auto_test_loss.png}
      \caption{Test set loss of autoencoders with and without feedback on the CIFAR-10 dataset. Neither model makes use of batch normalisation. }
      \label{fig:cifarauto}
  \end{figure}
  
  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/cifar_auto_batch_test_loss.png}
      \caption{Test set loss of autoencoders with and without feedback on the CIFAR-10 dataset. Both models make use of batch normalisation. }
      \label{fig:cifarautobatch}
  \end{figure}

\subsubsection{Noisy Activations}
\label{noisycifar}
The experiment from section \ref{noisyacts} is repeated on the CIFAR-10 dataset. The network employed is the autoencoder without batch normalisation from the previous experiment. 

Since feedback increased the robustness to noise in the MNIST autoencoder, the same behaviour would be expected here. However, as apparent in figure \ref{fig:cifarnoise}, the network with feedback is much more sensitive to (even small amounts of) noise than the one without feedback. 

This may be an indication that the feedback learned by the network is fundamentally different from the feedback learned in the MNIST experiments, such that it has a compounding effect on noise, rather than a rectifying one. 

\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/cifar_actnoise.png}
      \caption{Gaussian noise with zero mean and varying standard deviations is added to the CIFAR-10 autoencoders with and without feedback. Although this is not apparent due to the scale of the plot, the data for the network without feedback follows a similar shape to the one with feedback.}
      \label{fig:cifarnoise}
  \end{figure}

\subsubsection{Classification} 
\label{cifarclass}
Classification on the CIFAR-10 dataset is performed using a convolutional neural network. The network consists of two convolutional layers with 64 filters of size $5 \times 5$, each followed by a max pooling \cite{zhou1988computation} layer with a $2\times2$ window and a stride of $2$. The convolution  and pooling layers are followed by a fully connected layer (200 units) and a softmax \cite{bridle1990probabilistic} layer. 
Batch normalisation is applied after the pooling layers and dropout with a rate of $0.5$ is applied after the pooling and the fully connected layers. 

To test whether feedback can improve classification performance, the network is trained with and without feedback. Figure \ref{fig:cifarclass} shows only a marginal performance difference between the two networks, with the feedback network being slightly better. 

Note that the network employed here makes use of batch normalisation, which, as shown in the previous section, may be problematic in combination with feedback. Whether this is the case here is not clear, since this particular network will not converge when batch normalisation is disabled (be it with or without feedback). 

\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/cifar_class_test_loss.png}
      \caption{Classification loss on the CIFAR-10 test set. The training time of 200000 batches corresponds to 512 epochs.}
      \label{fig:cifarclass}
  \end{figure}
  
\section{Conclusion}
tell them what you told them 

The feedback mechanism presented here is able to improve performance of conventional networks both in terms of convergence speed and convergence value in certain cases. The effectiveness of the mechanism is inconsistent across different datasets, with ...
This allows two conclusions: (1) the effectiveness of the mechanism is data-dependent, i.e. it may be leveraging the highly regular structure of the MNIST dataset, ... , or (2) the implementation of the mechanism in the CIFAR-10 experiments is not .... Notably, in convolutional networks, feedback is given on a filter-wise basis, which may be too simplistic.  


Noise


Section \ref{cifarae} suggests that there may be an (unfavourable) interaction between the feedback mechanism and batch normalisation. Investigating why this is the case may shed further light on how the feedback works and what role it fulfils. 



suggest future work: relationship feedback batchnorm 

multi modal 


\bibliography{example_paper}
\bibliographystyle{icml2017}

\newpage
\clearpage

\section{Appendix}

\subsection{Hyperparameter Tuning} 
As mention in section \ref{mnist}, optimal values for $\beta_{max}$ and $\eta$ are determined by a grid search. The initial grid is defined by $\eta = [5, 10, 15, \dots, 50]$ and $\beta_{max} = [0.1, 0.2, \dots, 0.8]$. 

The highest value for $\beta_{max} \ (0.8)$ consistently shows the best performance regardless of $\eta$'s values, as exemplified by figure \ref{fig:hyper1}. Note that a high constant value of $\eta$ with varying values of $\beta_{max}$ will generally lead to less spread between the loss curves, since the activation function will be more sensitive to $\beta_{max}$ when $\eta$ is low. 

\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/hyper/eta5.png}
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/hyper/eta50.png}
      \caption{Top: first pass, bottom: second pass}
      \label{fig:hyper1}
  \end{figure}

While higher values of $\beta_{max}$ lead to better performance, the inverse relationship can be seen with $\eta$, i.e. lower values of $\eta$ lead to better performance. This is illustrated in figure \ref{fig:hyper2}.  

\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/hyper/betamax08.png}
      
      \caption{Top: first pass, bottom: second pass}
      \label{fig:hyper2}
 \end{figure}
 
A second grid search with $\eta = [1, 2, 3, 4, 5], \beta_{max} = [0.8, 0.85, 0.9, 0.95]$ is performed to determine whether even lower/higher values can further improve performance. Indeed, increasing $\beta_{max}$ to $0.95$ leads to better performance, but further decreasing $\eta$ is not advantegeous. 

\subsection{Visualising Activations} 
\begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,keepaspectratio]{img/appendix/actvis/firstpass.png}
      \includegraphics[width=0.5\textwidth,keepaspectratio]{img/appendix/actvis/secondpass.png}
      \caption{Top: first pass, bottom: second pass}
      \label{fig:actvis}
  \end{figure}



\subsection{Feedback-Controlled Threshold} 
Equation \ref{eq:neurogain} describes not only gain modulation through feedback, but also an adjustment of the activation functions threshold, i.e. $\alpha \mu_D$ is one of the terms in the summation. While gain modulation is the main property of interest in this paper, it is conceivable that the change in threshold plays a significant part in this mechanism as well. 

Incorporating this threshold mechanism into equation \ref{eq:fgrelu} leads to: 
\begin{equation}
	f = \frac{max(0, \mu_S + \alpha \mu_D)}{1 - min(\frac{\beta_{max}}{\eta} \ \mu_D, \beta_{max})} 
\end{equation}

where $\alpha$ is a parameter to be learned by the network. While $\alpha$ could also be set to a constant (tuned) value, prior experiments suggest that it is beneficial to let the network adjust $alpha$ during the course of training. 

As can be seen in figure \ref{fig:threshold}, the added threshold mechanism is not able to improve upon the network implementing the gain mechanism. Although the models with feedback-controlled threshold both perform better than the standard autoencoder, the model with only gain and no threshold mechanism still has the overall best performance. 

\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/threshold/threshold_test.png}
            \caption{Performance of the standard autoencoder, an autoencoder with feedback-controlled threshold, an autoencoder with feedback-controlled gain, and an autoencoder with both feedback-controlled threshold and gain on the MNIST test set.}
      \label{fig:threshold}
  \end{figure}

\subsection{T-SNE} 
\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/mnist_tsne/firstpass.png}
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/mnist_tsne/secondpass.png}
      \caption{T-SNE visualisation of the second encoding layer of the autoencoder over the whole MNIST test set. Top: first pass, bottom: second pass}
      \label{fig:tsne}
  \end{figure}
  
  \begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/mnist_tsne/noisefirstpass.png}
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/mnist_tsne/noisesecondpass.png}
      \caption{T-SNE visualisation of the second encoding layer of the autoencoder over the whole MNIST test set. Top: first pass, bottom: second pass}
      \label{fig:tsnenoise}
  \end{figure}

  
  
  
 \subsection{Input With Reduced Contrast}
Images with reduced contrast are presented to the trained (on regular contrast images) network, to see if the second pass can reconstruct an image that is more akin to a regular contrast image. To reduce the contrast, each pixel of the image is multiplied by some contrast factor $0 \leq c \leq 1$. 

Figure \ref{fig:contrastdiff} shows the absolute difference in mean pixel value between the first and second pass reconstructions for a number of different contrast factors. A high contrast input image leads to a larger difference in mean pixel value, while a low contrast image leads to a smaller difference between first and second pass reconstructions. 




\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/contrast/diff.png}
            \caption{Absolute difference in mean pixel value between first and second pass reconstructions as a function of different contrast factors (from $0.0$ to $1.0$ in $0.1$ increments). A contrast factor of $1.0$ corresponds to no reduction in contrast, while a contrast factor of $0.0$ means the input images are entirely black.}
      \label{fig:contrastdiff}
  \end{figure}
  
  
  \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/contrast/contrastrec.png}
            \caption{From top to bottom: original image, contrast reduced image, first pass reconstruction, second pass reconstruction. The contrast reduced image was produced by multiplying the original image with a contrast factor of $0.5$, i.e. each pixel in the contrast reduced image has values in the range $[0.0, 0.5]$ instead of $[0.0, 1.0]$}
      \label{fig:contrastdiff}
  \end{figure}

\subsection{Additional plots for section \ref{cifarclass} (CIFAR-10 Classification)}
\begin{figure}[H]
      \centering
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/cifarclass/testacc.png}
      
      \includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/cifarclass/trainloss.png}

	\includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/cifarclass/trainacc.png}
	
	\includegraphics[width=0.5\textwidth,height=5cm,keepaspectratio]{img/appendix/cifarclass/trainlossma.png}


            \caption{CIFAR-10 classification as seen in section \ref{cifarclass}. From top to bottom: test set accuracy, training set loss, training set accuracy, training set loss after applying a moving average filter (window size 100). }
      \label{fig:addcifarclass}
  \end{figure}


  
  
 \subsection{Reflections} 
 
 material learned: got more comfortable with neural nets (theory - practice divide), initialisations, batchnorm, data normalisation 
 
 

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
